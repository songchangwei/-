最近邻分类属于 基于实例的学习 或 非泛化学习 ：它不会去构造一个泛化的内部模型，而是简单地存储训练数据的实例。 
分类是由每个点的最近邻的简单多数投票中计算得到的：一个查询点的数据类型是由它最近邻点中最具代表性的数据类型来决定的。
k-近邻算法是最近邻分类的技术中比较常用的一种。 
值的最佳选择是高度依赖数据的：通常较大的 k 是会抑制噪声的影响，但是使得分类界限不明显。
基本的最近邻分类使用统一的权重：分配给查询点的值是从最近邻的简单多数投票中计算出来的。
在某些环境下，最好对邻居进行加权，使得更近邻更有利于拟合。
sklean中KNN的使用
def KNN():
    iris = datasets.load_iris()
    train_X,test_X,train_y,test_y = train_test_split(iris.data,iris.target,test_size=0.33, random_state=42)
    knn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto',metric='minkowski')
    knn.fit(train_X,train_y)
    result = knn.predict(test_X)
    print(accuracy_score(test_y,result))
