最近邻方法背后的原理是从训练样本中找到与新点在距离上最近的预定数量的几个点，然后从这些点中预测标签。 

这些点的数量可以是用户自定义的常量（K-最近邻学习）， 也可以根据不同的点的局部密度（基于半径的最近邻学习）确定。

距离通常可以通过任何度量来衡量： standard Euclidean distance（标准欧式距离）是最常见的选择。

尽管它简单，但最近邻算法已经成功地适用于很多的分类和回归问题，例如手写数字或卫星图像的场景。 

作为一个 non-parametric（非参数化）方法，它经常成功地应用于决策边界非常不规则的分类情景下。

使用sklearn中的NearestNeighbors实现最近邻算法

#!/usr/bin/python

from sklearn.neighbors import NearestNeighbors

import numpy as np

X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)

distances, indices = nbrs.kneighbors(X)

print(distances)

print(indices)

结果：[[0.         1.        ]

 [0.         1.        ]
 
 [0.         1.41421356]
 
 [0.         1.        ]
 
 [0.         1.        ]
 
 [0.         1.41421356]]
 
 
[[0 1]

 [1 0]
 
 [2 1]
 
 [3 4]
 
 [4 3]
 
 [5 4]]

注意这里原数据和要求的数据都是X。

k值为2。也就是求距离改点最近的两个点

结果中，第i表示第i个点，induce中第i行表示距离改点最近的两个点，且第一个为本身，distane中第i行表示距离改点最近的两个的距离，第一个是本身。
